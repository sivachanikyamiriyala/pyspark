import os
import time
import json
import requests
import logging 
import readline 
from pyspark.sql import SparkSession,HiveContext
from pyspark.sql import * 
from pyspark.sql import functions 
from pyspark.sql.functions import udf 
from pyspark.sql.functions import *
from pyspark.sql.types import *
import requests
parkdriver=SparkSession.builder.master('local').appName('de').enableHiveSupport().config('spark.jars.packages','mysql:mysql-connector-java:5.1.45').getOrCreate() 
															  
df_mysql=sparkdriver.read.format('jdbc').option('url','jdbc:mysql://localhost:3306').\
                                                              option('driver','com.mysql.jdbc.Driver').\
															  option('user','root').\
															  option('password','hadoop').\
															  option('query','select * from kalyan.orders limit 10000').load()
df_mysql.count()
df_mysql.withColumn("day",current_date()).show()
df_mysql2=df_mysql.withColumn("day",current_date())
df_mysql2.write.mode('append').saveAsTable("kalyan.ssri")
df_mysql2.write.mode('append').partitionBy("day").mode('append').saveAsTable("kalyan.sri")
df_mysql3=df_mysql.withColumn("day",date_add(current_date(),1))
df_mysql.show(1)
df_mysql3.show(1)
df_mysql3.write.partitionBy("day").mode(
)
df_mysql3.write.partitionBy("day").mode('append').saveAsTable("kalyan.sri")
df_mysql.show()
df_mysql.count()
df=df_mysql.alias("df")
df.show()
df.count()
df.createOrReplaceTempView("sample")
sparkdriver.sql("select * from sample").count()
sparkdriver.sql("select * from sample").show()
sparkdriver.sql("select order_id,order_date from sample").show(4)
df.select("order_date",df.order_id).show(4)
saprkdriver.sql("select order_id,order_date from sample where order_id>1000 and order_id<1010").show()
sparkdriver.sql("select order_id,order_date from sample where order_id>1000 and order_id<1010").show()
df.select("order_id","order_date").where("order_id>1000" and "order_id<1010").show()
df.select("order_id","order_date").filter("order_id>1000" and "order_id<1010").show()
df.select("order_id","order_date").filter(df.order_id>1000 and df.order_id<1010df.select("order_id").show()
df.select("order_id","order_date").where("order_id>1000").show()
df.select("order_id","order_date").where("order_id>1000").filter("order_id<1010").show()
sparkdriver.sql("select order_status,count(1) from sample group by order_status").show()
df.groupBy("order_status").count().show()
sparkdriver.sql("select substr(order_status,1,1) as ch,count(1) from sample group by ch having count(1) >10000").show()
sparkdriver.sql("select substr(order_status,1,1) as ch,count(1) from sample group by ch having count(1)>10000").show()
sparkdriver.sql("select substr(order_status,1,1) as ch,count(1) from sample group by ch having count(1)>1000").show()
df.groupBy(df.order_status.substr(1,1)).count().show()
df.groupBy(df.order_status.substr(1,1)).count().filter(count>1000).show()
df.groupBy(df.order_status.substr(1,1)).count().filter(col("count>1000")).show()
df.groupBy(df.order_status.substr(1,1)).count().show()
df.groupBy(df.order_status.substr(1,1)).agg(count("o))
sparkdriver.sql("select order_status,count(1),min(order_id),max(order_id) from sample group by order_status").show()
df.groupBy("order_status").agg(count("order_status")).show()
df.groupBy("order_status").agg(count("order_status"),min("order_id").max("order_id")).show()
df.groupBy("order_status").agg(count("order_status"),min("order_id").max("order_id"))).show()
df.groupBy("order_status").agg(count("order_status")).show()
df.groupBy("order_status").agg(count("order_status"),min("order_id")).show()
df.groupBy("order_status").agg(count("order_status"),min("order_id"),max("order_id")).show()
df.withColumn("order_status",when(df.order_status=="COMPLETE","OK")).show()
df.withColumn("order_status",when(df.order_status=="COMPLETE","OK").when(df.order_status="CLOSED","GOOD")).show()
df.withColumn("order_status",when(df.order_status=="COMPLETE","OK").when(df.order_status="CLOSED","GOOD"))).show()
df.withColumn("status",when(df.order_status=="COMPLETE","OK")).show()
df.withColumn("status",when(df.order_status=="COMPLETE","OK").when(df.order_status=="PROCESSING","WAIT")).show()
df.withColumn("status",when(df.order_status=="COMPLETE","OK").when(df.order_status=="PROCESSING","WAIT").otherwise("ok")).show()
df1=df.withColumn("status",when(df.order_status=="COMPLETE","OK").when(df.order_status=="PROCESSING","WAIT").otherwise("ok"))
df.printSchema()
df1.printSchema()
df.drop("order_id","order_date").show()
df.dropDuplicates(['order_status']).show()
df.select("order_status").distinct().show()
df.cube("order_id","order_status").count().show()
df.cube("order_id","order_status").count().na.replace(366,99999,'order_id').show()
df.cube("order_id","order_status").count().na.replace(366,99999,'order_id').na.replace("CLOSED","OK",'order_status').show()
df.rollup("order_id","order_status").count().na.replace(366,99999,'order_id').na.replace("CLOSED","OK",'order_status').show()
df.orderBy(desc("order_id")).show(10)
df.selectExpr("order_id*100 as sivaid").show(4)
df.select("order_id","order_status").show()
df.filter("order_status='complete'").show()
df.filter("order_status='complete'" or "order_status=='CLOSED'").show()
df.filter("order_status='complete'" or "order_status=='CLOSED'").count()
df.filter("order_status='complete'").count()
df.filter("order_status='PROCESSING'" or "order_status=='CLOSED'").count()
df.withColumn("newc",lit(200)).show()
df.withColumn("newc",lit(200)).withColumnRenamed("order_date","date").show()
df.count()
df1=df.where("order_id%2==0")
df2=df.where("order_id%2==1")
df1.count()
df2.count()
df1.printSchema()
df2.printSchema()
df.select("order_id","order_status").join(df2.select("order_id","order_date"),df1.order_status==df2.order_status,'inner').count()
df.join(df2,df1.order_status==df2.order_status,'inner').count()
df.select("order_id").join(df2.select("order_id"),df1.order_status==df2.order_status,'inner').count()
df1.select("order_id").join(df2.select("order_id"),df1.order_status==df2.order_status,'inner').count()
df.join(df2,df1.order_status==df2.order_status,'inner').count()
df.join(df2,df1.order_status==df2.order_status,'left_outer').count()
df.join(df2,df1.order_status==df2.order_status,'right_outer').count()
df.join(df2,df1.order_status==df2.order_status,'full_outer').count()
df6=df1.limit(10)
df9=df2.limit(10)
df6.count()
df9.count()
df6.join(df9,df6.order_status==df9.order_status,'inner').show()
df6.select("order_date","order_id").join(df9.select("order_status","order_id"),df6.order_status==df9.order_status,'inner').show()
df6.show()
df9.show()
df6.join(df9,df6.order_status==df9.order_status,'inner').show()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'inner').show()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'left_outer').show()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'right_outer').show()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'full_outer').show()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'full_outer').count()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'left__outer').count()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'right__outer').count()
df6.select(df6.order_id,df6.order_status).join(df9.select(df9.order_date,df9.order_status),df6.order_status==df9.order_status,'inner').count()
print(dir(readline))
readline.write_history_file('/home/spark/history1')
df6.createGlobalTempView("siva1")
sparkdriver.sql("select * from 
sparkdriver.sql("select * from global_temp.siva1").show()
spark1=sparkdriver.newSession()
spark1.sql("select * from global_temp.siva1").show()
df6.write.format('csv').option('delimiter','\t').option("header","false").mode('overwrite').save('/home/spark/siva_csv')
df6.write.format('json')..mode('overwrite').save('/home/spark/siva_json')
df6.write.format('json').mode('overwrite').save('/home/spark/siva_json')
df6.write.format('parquet').mode('overwrite').save('/home/spark/siva_parquet')
df6.write.format('orc').mode('overwrite').save('/home/spark/siva_orc')
df_csv=sparkdriver.read.format('csv').option('delimiter','\t').option("header","true").load('/home/spark/siva_csv').show(10)
df_csv=sparkdriver.read.format('csv').option('delimiter','\t').option("header","true").load('/home/spark/siva_csv')
df_csv.show()
df_json=sparkdriver.read.format('json').load('/home/spark/siva_json').show(10)
df_json=sparkdriver.read.format('json').load('/home/spark/siva_json')
df_json.show()
df_parquet=sparkdriver.read.load('/home/spark/siva_parquet').show()
df_parquet=sparkdriver.read.load('/home/spark/siva_parquet')
#by default it will read as parquet and write also parquet 
df_parquet.show()
df_orc.show()
df_orc=sparkdriver.read.format('orc').load('/home/spark/siva_orc')
df_orc.show()
df_csv.show()
sch=StructType([StructField("id",LongType()),StructField("date1",TimestampType()),StructField("cid",LongType()),StructField("status",StringType())])
df_csv=sparkdriver.read.format('csv').option('delimiter','\t').schema(sch).load('/home/spark/siva_csv')
df_csv.show()
df10=df6.withColumn("day",current_date())
df10.show()
df10.write.format('csv').option('delimiter','\t').partitionBy("day").mode('append').save('hdfs://localhost:8020/spark/partitonByday')
df10=df6.withColumn("day",date_add(current_date(),1))
df10.write.format('csv').option('delimiter','\t').partitionBy("day").mode('append').save('hdfs://localhost:8020/spark/partitonByday')
df10.
df10.show()
df10.write.option('path','hdfs://localhost:8020/spark/chanikya').mode('append').saveAsTable("kalyan.chanikya")
df10=df6.withColumn("day",date_add(current_date(),2))
df10.write.option('path','hdfs://localhost:8020/spark/chanikya').mode('append').saveAsTable("kalyan.chanikya")
df10.write.partitionBy("day").mode('append').saveAsTable("kalyan.chanikya1")
df10=df6.withColumn("day",date_add(current_date(),3))
df10.write.partitionBy("day").mode('append').saveAsTable("kalyan.chanikya1")
sparkdriver.sql("show databases").show()
sparkdriver.sql("show tables").show()
sparkdriver.sql("show tables in default").show()
sparkdriver.sql("show tables in kalyan").show()
df10.show()
df10.write.format('jdbc').partitionBy("day").options(url='jdbc:mysql://localhost:3306',driver='com.mysql.jdbc.Driver',user='root',password='hadoop',dbtable='kalyan.sriram').save()
df10=df6.withColumn("day",date_add(current_date(),4))
df10.write.format('jdbc').partitionBy("day").options(url='jdbc:mysql://localhost:3306',driver='com.mysql.jdbc.Driver',user='root',password='hadoop',dbtable='kalyan.sriram').mode('append').save()
rdd_df10=df10.rdd
print(type(rdd_df10))
rdd_df10.collect()
df11=sparkdriver.createDataFrame(rdd_df10,sch)
df1..show()
df11.show()
df11=sparkdriver.createDataFrame(rdd_df10,"sch")
sch
df11=sparkdriver.createDataFrame(rdd_df10)
df11.show()
sch1=StructType([StructField("id",LongType()),StructField("date2",TimestampType()),StructField("cid",LongType()),StructField("stat",StringType())])
rdd_df10.take(1)
df12=sparkdriver.createDataFrame(rdd_df10,sch1)
df12.show()
df12=sparkdriver.createDataFrame(rdd_df10)
df12.show()
rd1=sparkdriver.sparkContext.textFile('/home/spark/sriram')
rd1.take(1)
print(type(rd1))
print(type(rd1.collect()))
rd2=rd1.map(lambda x:(str)(x))
rd2.collect()
rd3=rd2.map(lambda x:x.split("##"))
rd3.take(1)
print(type(rd3.collect()))
rd4=rd3.map(lambda x:(x[0],x[1],x[2],x[3],x[4]))
rd4.collect()
rd4.take(1)
df20=rd4.toDF()
df20.show()
df20=rd4.toDF(["id","id1","id2","id3","id4"])
df20.show(1)
df20.printSchema()
df20.collect()
rd1=spark.sparkContext('/home/spark/sriram')
rd1=sparkdriver.sparkContext('/home/spark/sriram')
rd1=spark.sparkContext.textFile('/home/spark/sriram')
rd2=rd1.map(lambda x:(str)(x))
rd3=rd2.map(lambda x:x.split("##"))
rd4=rd3.map(lambda x:(x[0],x[1],x[2]))
df20=rd4.toDF()
df20.printSchema()
df20.show()
rd1=sparkdriver.sparkContext.textFile('/home/spark/sriram')
print(type(rd1.collect()))
print(type(rd1))
rd2=rd1.map(lambda x:(str)(x))
rd2.take(2)
print(type(rd2.take(2)))
rd3=rd2.map(lambda x:x.split("##"))
rd3.take(2)
rd4=rd3.map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5]))
rd4.take(1)
rd4.take(2)
df11=rd4.toDF()
df11.show(1)
df11=rd4.toDF(["id1","id2","id3","id4","id5","id6"])
df11.show(1)
df11.show(2)
import requests 
jsonapidata=requests.request('GET','https://api.github.com/')
jsondata=jsonapidata.json()
file=open('/home/spark/ssriram','a')
for record in jsondata:
    file.write("%S\n" %record)
for record in jsondata:
    file.write(record)
for record in jsondata:
    file.write("%s\n" %record)
file=open('/home/spark/ssriram','a')
for record in jsondata:
    file.write("%s\n" %record)
file.close()
file=open('/home/spark/ssriram','a')
for record in jsondata:
    file.write("%s\n" %record)
import requests 
jsonapidata=requests.request('GET','https://api.github.com/')
jsondata=jsonapidata.json()
file=open('/home/spark/ssriram','a')
for record in jsondata:
    file.write("%s\n" %record)
rdd=sparkdriver.sparkContext.textFile('/home/spark/chandraanna')
rdd.take(1)
rdd.take(2)
rdd.collect()
df=sparkdriver.read.format('json').option('multiLine',True).load('/home/spark/chandraanna')
df.printSchema()
jsonapidata=requests.request('GET','https://api.github.com/users/hadley/orgs')
jsondata=jsonapidata.json()
file=open('/home/spark/chandraanna1','a')
for record in jsondata:
    file.write("%s\n" %record)
file.close()
df1=sparkdriver.read.format('json').option('multiLine',True).load('/home/spark/chandraanna1').printSchema()
df1=sparkdriver.read.format('json').option('multiLine',True).option('delimiter','\t').load('/home/spark/chandraanna1').printSchema()
import requests
import time
from pyspark.sql import *
sparkdriver=SparkSession.builder.master('local').appName('demo').\
                                                    getOrCreate() 
													
jsonapidata=requests.request('GET','https://api.github.com/users/hadley/orgs')
jsondata=jsonapidata.json()
file=open('/home/spark/teja1','a')													
for record in jsondata:
    file.write((str)(record))
df_teja1=sparkdriver.read.format('json').load('/home/spark/teja1')
df_teja1.printSchmea()
df_teja1.printSchema()
jsonapidata=requests.request('GET','https://api.github.com/users/hadley/orgs')
file=open('/home/spark/teja2','a')													
for record in jsondata:
    file.write("%s\n" %record)
df_teja2=sparkdriver.read.format('json').load('/home/spark/teja2').printSchema()
file.close()
df_teja2=sparkdriver.read.format('json').load('/home/spark/teja2').printSchema()
df_teja2=sparkdriver.read.format('json').option('multiLine',True).load('/home/spark/teja2').printSchema()
df_teja2=sparkdriver.read.format('json').option('multiLine',True).option('delimiter','\t').load('/home/spark/teja2').printSchema()
file=open('/home/spark/teja3','a')													
for record in jsondata:
    file.write("%s" %record)
df_teja3=sparkdriver.read.format('json').load('/home/spark/teja2').printSchema()
file=open('/home/spark/teja4','a')													
for record in jsondata:
    file.write((str)(record))
df_teja4=sparkdriver.read.format('json').load('/home/spark/teja4').printSchema()
readline.write_history_file('/home/spark/pyspark/realreal')
